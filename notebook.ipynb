{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting \"[Ww]hales?\" in *Moby Dick*\n",
    "\n",
    "Just for fun, we will count the number of times the word *whale* or *Whale*, or plurals thereof, occur in the book *Moby Dick*. We'll use two methods to do this. But there are plenty of other ways to do it than these two here. Some other ways might be worse, but I'm sure there are some other ways that are better too. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib.request as ur\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# An nltk add-on module. This will download it, if you don't have it already.\n",
    "_ = nltk.download('punkt', quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the *Moby Dick* text\n",
    "\n",
    "Get the text of *Moby Dick* from Project Gutenberg. We'll just read the whole file in as one big (multi-line) string. This file will have Project Gutenberg's boilerplate at the start and at the end, and we'll strip that out. Using a regular expression pattern, we'll find the string starting with \"Call me Ishmael\" and including any characters (including line breaks) until we hit \"End of Project Gutenberg's Moby Dick\". We'll call this string `mobydick`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fulltext = ur.urlopen(\"http://www.gutenberg.org/ebooks/2701.txt.utf-8\").read()\n",
    "\n",
    "mobydick = re.search(b\"(Call me Ishmael.*)End of Project Gutenberg's Moby Dick.*$\",\n",
    "                     fulltext, re.S).groups()[0].strip().decode('utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define some regular expressions\n",
    "\n",
    "We can define a regular that matches all and only the following strings: *whale*, *whales*, *Whale* and *Whales*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "whale_regex = re.compile('^[Ww]hales?$')\n",
    "is_whale_word = lambda word: True if whale_regex.match(word) else False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also make some regular expressions that match any punctuation character, and non empty strings of nothing but punctuation characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "punctuation_regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "punctuation_string_regex = re.compile('^[%s]+$' % re.escape(string.punctuation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some helper functions\n",
    "\n",
    "We'll count the whale words by tokenizing the text and then find the whale words using the regular expression above.\n",
    "\n",
    "For convenience, we'll define a function that count whale words in a list of words, and also a general tokenize and count function, and a function to summarize our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_whale_words(words):\n",
    "    \"Return a list of all words that match the whale_regex\"\n",
    "    return [word for word in words if is_whale_word(word)]\n",
    "\n",
    "def tokenize_and_count(text, tokenizer):\n",
    "    '''Return a dictionary with all words in `text` tokenized and a list \n",
    "    of whale words too.'''\n",
    "    \n",
    "    results = dict(all_words = tokenizer(text))\n",
    "    results['whale_words'] = get_whale_words(results['all_words'])\n",
    "    \n",
    "    return results\n",
    "\n",
    "def summarize_results(results):\n",
    "    \"\"\"Return a string summarizing the results of a the whale word search.\"\"\"\n",
    "    \n",
    "    whale_words, all_words = (results['whale_words'], results['all_words'])\n",
    "    \n",
    "    count_breakdown = \"\\n\".join(['%s: %s' % (string, count) \n",
    "                           for string, count in Counter(whale_words).items()])\n",
    "    \n",
    "    summary = \"There are %d whale words in %d words. This is a ratio of %2.5f.\" % (\n",
    "        len(whale_words), len(all_words), len(whale_words)/len(all_words)\n",
    "    )\n",
    "\n",
    "    return \"\\n\".join([summary,\n",
    "                      \"The matching words breakdown as follows:\",\n",
    "                      count_breakdown])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize using *Natural Language Toolkit*\n",
    "\n",
    "We'll tokenize `mobydick` using the *Natural Language Toolkit*. This will use a built in tokenizer to return a list of what it defines as \"words\". This will treat things like \".\" and \"--\", and so on, as words. We can filter out these punctuation character strings by definining a new regular expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def nltk_tokenize_and_count(text):\n",
    "    \n",
    "    nltk_tokenizer = lambda text: [word for word in word_tokenize(text)\n",
    "                                   if not punctuation_string_regex.match(word)]\n",
    "\n",
    "    return tokenize_and_count(text, nltk_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nltk_results = nltk_tokenize_and_count(mobydick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1242 whale words in 212164 words. This is a ratio of 0.00585.\n",
      "The matching words breakdown as follows:\n",
      "whale: 741\n",
      "whales: 221\n",
      "Whales: 23\n",
      "Whale: 257\n"
     ]
    }
   ],
   "source": [
    "print(summarize_results(nltk_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize the old school way\n",
    "\n",
    "The old unix way to tokenize is to remove your punctuation characters and then split the remain string by whitespaces and line breaks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def oldschool_tokenize_and_count(text):\n",
    "    \n",
    "    old_school_tokenizer = lambda text: punctuation_regex.sub('', text).split()\n",
    "\n",
    "    return tokenize_and_count(text, old_school_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "oldschool_results = oldschool_tokenize_and_count(mobydick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1240 whale words in 208420 words. This is a ratio of 0.00595.\n",
      "The matching words breakdown as follows:\n",
      "whale: 663\n",
      "whales: 296\n",
      "Whales: 66\n",
      "Whale: 215\n"
     ]
    }
   ],
   "source": [
    "print(summarize_results(oldschool_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which method is better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The counts from the two methods are quite close, but the breakdowns are obviously different. We see, for example, more *whales* and *Whales* counts in the old school way than in the nltk way. Why is this? Without looking into it more carefully, I presume it is to do with the possesive case words like \"whale's\". The old school tokenizer will strip out the \"'\" and leave the words as \"whales\", while the nltk tokenize will treat \"whale's\" as two separate words, \"whale\" and \"'s\", which is actually correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a simple example for comparison. Here it is easy to see how the discrepancies arise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Whales are big fish. Actually, a whale is not a fish at all. \n",
    "A whale's tail is the tail of a whale.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4 whale words in 23 words. This is a ratio of 0.17391.\n",
      "The matching words breakdown as follows:\n",
      "whale: 3\n",
      "Whales: 1\n"
     ]
    }
   ],
   "source": [
    "print(summarize_results(nltk_tokenize_and_count(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4 whale words in 22 words. This is a ratio of 0.18182.\n",
      "The matching words breakdown as follows:\n",
      "whale: 2\n",
      "whales: 1\n",
      "Whales: 1\n"
     ]
    }
   ],
   "source": [
    "print(summarize_results(oldschool_tokenize_and_count(text)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
